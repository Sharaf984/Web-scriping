{
    "cells": [
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#spider.py\nimport sqlite3\nimport urllib.error\nimport ssl\nfrom urllib.parse import urljoin\nfrom urllib.parse import urlparse\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nconn = sqlite3.connect('spider.sqlite')\ncur = conn.cursor()\n\ncur.execute('''CREATE TABLE IF NOT EXISTS Pages\n    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT,\n     error INTEGER, old_rank REAL, new_rank REAL)''')\n\ncur.execute('''CREATE TABLE IF NOT EXISTS Links\n    (from_id INTEGER, to_id INTEGER, UNIQUE(from_id, to_id))''')\n\ncur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')\n\n# Check to see if we are already in progress...\ncur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\nrow = cur.fetchone()\nif row is not None:\n    print(\"Restarting existing crawl.  Remove spider.sqlite to start a fresh crawl.\")\nelse :\n    starturl = input('Enter web url or enter: ')\n    if ( len(starturl) < 1 ) : starturl = 'http://www.dr-chuck.com/'\n    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n    web = starturl\n    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n        pos = starturl.rfind('/')\n        web = starturl[:pos]\n\n    if ( len(web) > 1 ) :\n        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) )\n        conn.commit()\n\n# Get the current webs\ncur.execute('''SELECT url FROM Webs''')\nwebs = list()\nfor row in cur:\n    webs.append(str(row[0]))\n\nprint(webs)\n\nmany = 0\nwhile True:\n    if ( many < 1 ) :\n        sval = input('How many pages:')\n        if ( len(sval) < 1 ) : break\n        many = int(sval)\n    many = many - 1\n\n    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n    try:\n        row = cur.fetchone()\n        # print row\n        fromid = row[0]\n        url = row[1]\n    except:\n        print('No unretrieved HTML pages found')\n        many = 0\n        break\n\n    print(fromid, url, end=' ')\n\n    # If we are retrieving this page, there should be no links from it\n    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n    try:\n        document = urlopen(url, context=ctx)\n\n        html = document.read()\n        if document.getcode() != 200 :\n            print(\"Error on page: \",document.getcode())\n            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n\n        if 'text/html' != document.info().get_content_type() :\n            print(\"Ignore non text/html page\")\n            cur.execute('DELETE FROM Pages WHERE url=?', ( url, ) )\n            conn.commit()\n            continue\n\n        print('('+str(len(html))+')', end=' ')\n\n        soup = BeautifulSoup(html, \"html.parser\")\n    except KeyboardInterrupt:\n        print('')\n        print('Program interrupted by user...')\n        break\n    except:\n        print(\"Unable to retrieve or parse page\")\n        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n        conn.commit()\n        continue\n\n    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) )\n    cur.execute('UPDATE Pages SET html=? WHERE url=?', (memoryview(html), url ) )\n    conn.commit()\n\n    # Retrieve all of the anchor tags\n    tags = soup('a')\n    count = 0\n    for tag in tags:\n        href = tag.get('href', None)\n        if ( href is None ) : continue\n        # Resolve relative references like href=\"/contact\"\n        up = urlparse(href)\n        if ( len(up.scheme) < 1 ) :\n            href = urljoin(url, href)\n        ipos = href.find('#')\n        if ( ipos > 1 ) : href = href[:ipos]\n        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n        if ( href.endswith('/') ) : href = href[:-1]\n        # print href\n        if ( len(href) < 1 ) : continue\n\n\t\t# Check if the URL is in any of the webs\n        found = False\n        for web in webs:\n            if ( href.startswith(web) ) :\n                found = True\n                break\n        if not found : continue\n\n        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) )\n        count = count + 1\n        conn.commit()\n\n        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n        try:\n            row = cur.fetchone()\n            toid = row[0]\n        except:\n            print('Could not retrieve id')\n            continue\n        # print fromid, toid\n        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) )\n\n\n    print(count)\n\ncur.close()\n",
            "execution_count": null,
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Enter web url or enter: \n['http://www.dr-chuck.com']\nHow many pages:3\n1 http://www.dr-chuck.com (8386) 2\n3 http://www.dr-chuck.com/sakai-book (5843) 4\n2 http://www.dr-chuck.com/dr-chuck/resume/index.htm (1855) 11\n"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#sprank.py\nimport sqlite3\n\nconn = sqlite3.connect('spider.sqlite')\ncur = conn.cursor()\n\n# Find the ids that send out page rank - we only are interested\n# in pages in the SCC that have in and out links\ncur.execute('''SELECT DISTINCT from_id FROM Links''')\nfrom_ids = list()\nfor row in cur: \n    from_ids.append(row[0])\n\n# Find the ids that receive page rank \nto_ids = list()\nlinks = list()\ncur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\nfor row in cur:\n    from_id = row[0]\n    to_id = row[1]\n    if from_id == to_id : continue\n    if from_id not in from_ids : continue\n    if to_id not in from_ids : continue\n    links.append(row)\n    if to_id not in to_ids : to_ids.append(to_id)\n\n# Get latest page ranks for strongly connected component\nprev_ranks = dict()\nfor node in from_ids:\n    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n    row = cur.fetchone()\n    prev_ranks[node] = row[0]\n\nsval = input('How many iterations:')\nmany = 1\nif ( len(sval) > 0 ) : many = int(sval)\n\n# Sanity check\nif len(prev_ranks) < 1 : \n    print(\"Nothing to page rank.  Check data.\")\n    quit()\n\n# Lets do Page Rank in memory so it is really fast\nfor i in range(many):\n    # print prev_ranks.items()[:5]\n    next_ranks = dict();\n    total = 0.0\n    for (node, old_rank) in list(prev_ranks.items()):\n        total = total + old_rank\n        next_ranks[node] = 0.0\n    # print total\n\n    # Find the number of outbound links and sent the page rank down each\n    for (node, old_rank) in list(prev_ranks.items()):\n        # print node, old_rank\n        give_ids = list()\n        for (from_id, to_id) in links:\n            if from_id != node : continue\n           #  print '   ',from_id,to_id\n\n            if to_id not in to_ids: continue\n            give_ids.append(to_id)\n        if ( len(give_ids) < 1 ) : continue\n        amount = old_rank / len(give_ids)\n        # print node, old_rank,amount, give_ids\n    \n        for id in give_ids:\n            next_ranks[id] = next_ranks[id] + amount\n    \n    newtot = 0\n    for (node, next_rank) in list(next_ranks.items()):\n        newtot = newtot + next_rank\n    evap = (total - newtot) / len(next_ranks)\n\n    # print newtot, evap\n    for node in next_ranks:\n        next_ranks[node] = next_ranks[node] + evap\n\n    newtot = 0\n    for (node, next_rank) in list(next_ranks.items()):\n        newtot = newtot + next_rank\n\n    # Compute the per-page average change from old rank to new rank\n    # As indication of convergence of the algorithm\n    totdiff = 0\n    for (node, old_rank) in list(prev_ranks.items()):\n        new_rank = next_ranks[node]\n        diff = abs(old_rank-new_rank)\n        totdiff = totdiff + diff\n\n    avediff = totdiff / len(prev_ranks)\n    print(i+1, avediff)\n\n    # rotate\n    prev_ranks = next_ranks\n\n# Put the final ranks back into the database\nprint(list(next_ranks.items())[:5])\ncur.execute('''UPDATE Pages SET old_rank=new_rank''')\nfor (id, new_rank) in list(next_ranks.items()) :\n    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\nconn.commit()\ncur.close()\n\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#spdump.py\nimport sqlite3\n\nconn = sqlite3.connect('spider.sqlite')\ncur = conn.cursor()\n\ncur.execute('''SELECT COUNT(from_id) AS inbound, old_rank, new_rank, id, url \n     FROM Pages JOIN Links ON Pages.id = Links.to_id\n     WHERE html IS NOT NULL\n     GROUP BY id ORDER BY inbound DESC''')\n\ncount = 0\nfor row in cur :\n    if count < 50 : print(row)\n    count = count + 1\nprint(count, 'rows.')\ncur.close()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#spjson.py\nimport sqlite3\n\nconn = sqlite3.connect('spider.sqlite')\ncur = conn.cursor()\n\n# Find the ids that send out page rank - we only are interested\n# in pages in the SCC that have in and out links\ncur.execute('''SELECT DISTINCT from_id FROM Links''')\nfrom_ids = list()\nfor row in cur: \n    from_ids.append(row[0])\n\n# Find the ids that receive page rank \nto_ids = list()\nlinks = list()\ncur.execute('''SELECT DISTINCT from_id, to_id FROM Links''')\nfor row in cur:\n    from_id = row[0]\n    to_id = row[1]\n    if from_id == to_id : continue\n    if from_id not in from_ids : continue\n    if to_id not in from_ids : continue\n    links.append(row)\n    if to_id not in to_ids : to_ids.append(to_id)\n\n# Get latest page ranks for strongly connected component\nprev_ranks = dict()\nfor node in from_ids:\n    cur.execute('''SELECT new_rank FROM Pages WHERE id = ?''', (node, ))\n    row = cur.fetchone()\n    prev_ranks[node] = row[0]\n\nsval = input('How many iterations:')\nmany = 1\nif ( len(sval) > 0 ) : many = int(sval)\n\n# Sanity check\nif len(prev_ranks) < 1 : \n    print(\"Nothing to page rank.  Check data.\")\n    quit()\n\n# Lets do Page Rank in memory so it is really fast\nfor i in range(many):\n    # print prev_ranks.items()[:5]\n    next_ranks = dict();\n    total = 0.0\n    for (node, old_rank) in list(prev_ranks.items()):\n        total = total + old_rank\n        next_ranks[node] = 0.0\n    # print total\n\n    # Find the number of outbound links and sent the page rank down each\n    for (node, old_rank) in list(prev_ranks.items()):\n        # print node, old_rank\n        give_ids = list()\n        for (from_id, to_id) in links:\n            if from_id != node : continue\n           #  print '   ',from_id,to_id\n\n            if to_id not in to_ids: continue\n            give_ids.append(to_id)\n        if ( len(give_ids) < 1 ) : continue\n        amount = old_rank / len(give_ids)\n        # print node, old_rank,amount, give_ids\n    \n        for id in give_ids:\n            next_ranks[id] = next_ranks[id] + amount\n    \n    newtot = 0\n    for (node, next_rank) in list(next_ranks.items()):\n        newtot = newtot + next_rank\n    evap = (total - newtot) / len(next_ranks)\n\n    # print newtot, evap\n    for node in next_ranks:\n        next_ranks[node] = next_ranks[node] + evap\n\n    newtot = 0\n    for (node, next_rank) in list(next_ranks.items()):\n        newtot = newtot + next_rank\n\n    # Compute the per-page average change from old rank to new rank\n    # As indication of convergence of the algorithm\n    totdiff = 0\n    for (node, old_rank) in list(prev_ranks.items()):\n        new_rank = next_ranks[node]\n        diff = abs(old_rank-new_rank)\n        totdiff = totdiff + diff\n\n    avediff = totdiff / len(prev_ranks)\n    print(i+1, avediff)\n\n    # rotate\n    prev_ranks = next_ranks\n\n# Put the final ranks back into the database\nprint(list(next_ranks.items())[:5])\ncur.execute('''UPDATE Pages SET old_rank=new_rank''')\nfor (id, new_rank) in list(next_ranks.items()) :\n    cur.execute('''UPDATE Pages SET new_rank=? WHERE id=?''', (new_rank, id))\nconn.commit()\ncur.close()\n\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#spreset.py\nimport sqlite3\n\nconn = sqlite3.connect('spider.sqlite')\ncur = conn.cursor()\n\ncur.execute('''UPDATE Pages SET new_rank=1.0, old_rank=0.0''')\nconn.commit()\n\ncur.close()\n\nprint(\"All pages set to a rank of 1.0\")\n",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}